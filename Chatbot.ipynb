{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI2GPO4kb7dc"
      },
      "source": [
        "# Building a Simple Chatbot from Scratch in Python (using NLTK)\n",
        "\n",
        "\n",
        "History of chatbots dates back to 1966 when a computer program called ELIZA was invented by Weizenbaum. It imitated the language of a psychotherapist: [Eliza](http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm?utm_source=ubisend.com&utm_medium=blog-link&utm_campaign=ubisend). \n",
        "\n",
        "On similar lines this is a very basic chatbot utlising the Python's NLTK library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EHD8BE4b7de"
      },
      "source": [
        "## Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdLNJtpub7de"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import random\n",
        "import string # to process standard python strings\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhe_tZ3qb7df",
        "outputId": "9e1284e7-4220-4a11-99c7-70c3185a5f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.4.1)\n",
            "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9NFRyyHb7dg"
      },
      "source": [
        "### Installing NLTK Packages\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kZKygFDb7dg",
        "outputId": "f9bdb297-9625-4f98-f18e-e34738734ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular', quiet=True) # for downloading packages\n",
        "#nltk.download('punkt') # first-time use only\n",
        "#nltk.download('wordnet') # first-time use only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-IckExHb7dh"
      },
      "source": [
        "## Reading in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d56SQYyXb7di"
      },
      "outputs": [],
      "source": [
        "f=open('chatbot.txt','r',errors = 'ignore')\n",
        "raw=f.read()\n",
        "raw = raw.lower()# converts to lowercase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjDaUCfGb7di"
      },
      "source": [
        "Basic text pre-processing includes:\n",
        "* Converting the entire text into **lowercase**\n",
        "* **Tokenization**\n",
        "\n",
        "_The NLTK data package includes a pre-trained Punkt tokenizer for English._\n",
        "\n",
        "* Removing **Noise**\n",
        "* Removing the **Stop words**. \n",
        "* **Stemming**\n",
        "* **Lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1x1MaJob7dj"
      },
      "source": [
        "## Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hooSPfIVb7dj"
      },
      "outputs": [],
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUbs1dKub7dk"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Define a function called LemTokens which will take as input the tokens and return normalized tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OlIWU_Hb7dk"
      },
      "outputs": [],
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzOoCVv3b7dk"
      },
      "source": [
        "## Keyword matching\n",
        "\n",
        "Next, we define a function for a greeting by the bot i.e if a user’s input is a greeting, the bot shall return a greeting response.ELIZA uses a simple keyword matching for greetings. Utilizing the same concept here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwl5QhJ6b7dk"
      },
      "outputs": [],
      "source": [
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYeo0O1ib7dl"
      },
      "source": [
        "## Generating Response\n",
        "\n",
        "### TF-IDF Approach\n",
        "\n",
        "**Term Frequency: is a scoring of the frequency of the word in the current document.**\n",
        "\n",
        "```\n",
        "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
        "```\n",
        "\n",
        "**Inverse Document Frequency: is a scoring of how rare the word is across documents.**\n",
        "\n",
        "```\n",
        "IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
        "```\n",
        "### Cosine Similarity\n",
        "\n",
        "Tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus\n",
        "\n",
        "```\n",
        "Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
        "```\n",
        "where d1,d2 are two non zero vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnAVjXsNb7dl"
      },
      "source": [
        "To generate a response from our bot for input questions, the concept of document similarity will be used. We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OehKoJIb7dl"
      },
      "outputs": [],
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpYMs-xDb7dm"
      },
      "source": [
        "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon user’s input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR9vd8gLb7dm",
        "outputId": "d605b2f0-efca-403e-af35-2f9b87aede8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
            "hi\n",
            "ROBO: hey\n",
            "hello\n",
            "ROBO: hey\n",
            "bye\n",
            "ROBO: Bye! take care..\n"
          ]
        }
      ],
      "source": [
        "flag=True\n",
        "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"ROBO: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! take care..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WspZgjy4b7dm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}